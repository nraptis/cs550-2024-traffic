I implemented and iteratively refined a convolutional neural network for traffic sign classification using the GTSRB dataset, working with 30×30 RGB images across 43 categories. Beginning from a simple baseline, I conducted systematic experiments to evaluate the impact of architectural and hyperparameter choices. I compared convolution kernel sizes of 3×3, 5×5, and 7×7, finding that stacked 3×3 kernels consistently outperformed larger kernels by better preserving local structure. I evaluated learning rates of 0.0005, 0.001, 0.0015, and 0.002, observing that a learning rate of 0.001 provided the most stable and accurate convergence. Regularization was explored through dropout rates of 0.25, 0.5, and 0.75, with a dropout rate of 0.5 yielding the best generalization. I experimented with one and two fully connected hidden layers and varied their widths, including 64, 128, and multiples of the number of categories, ultimately finding that a single hidden layer with a width of twice the number of categories performed best. I also compared Global Average Pooling to flattening of convolutional feature maps, determining that flattening substantially outperformed Global Average Pooling by retaining essential spatial information. Finally, I evaluated deeper architectures by adding a third convolution–pooling block, which led to reduced performance, indicating that additional depth was unnecessary for the small input resolution. The final model, consisting of two convolution–pooling blocks followed by flattening, a single dense layer, and dropout, achieved approximately 99% training accuracy and over 99% test accuracy, demonstrating that preserving spatial detail and carefully balancing model capacity were critical for this task.